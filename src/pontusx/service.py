import shutil
from datetime import datetime
from logging import Logger
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Tuple

from extended_dataset_profile.models.v0.edp import AssetReference, DataSpace, License, Publisher
from pydantic import HttpUrl, ValidationError

from edps import analyse_asset
from edps.compression.zip import ZipAlgorithm
from edps.file import sanitize_file_part
from edps.service import dump_service_info
from edps.taskcontextimpl import TaskContextImpl
from edps.types import Config, UserProvidedEdpData
from pontusx.args import Args
from pontusx.metadata import DDO, CustomData, read_ddo_file

PONTUSX_DS_NAME = "Pontus-X"
PONTUSX_DS_URL = "https://portal.pontus-x.eu"
PONTUSX_ASSET_BASE_URL = f"{PONTUSX_DS_URL}/asset"


def to_user_provided_edp_data(ddo: DDO) -> UserProvidedEdpData:
    publishDate = ddo.metadata.updated or ddo.metadata.created or datetime.now()

    try:
        license = License(url=str(HttpUrl(ddo.metadata.license)))
    except ValidationError:
        license = License(name=ddo.metadata.license)

    return UserProvidedEdpData(
        assetRefs=[
            AssetReference(
                assetId=ddo.id,
                dataSpace=DataSpace(
                    name=PONTUSX_DS_NAME,
                    url=PONTUSX_DS_URL,
                ),
                assetUrl=HttpUrl(f"{PONTUSX_ASSET_BASE_URL}/{ddo.id}"),
                publisher=Publisher(
                    name=ddo.metadata.author,
                ),
                publishDate=publishDate,
                license=license,
            )
        ],
        name=ddo.metadata.name,
        description=ddo.metadata.description,
        tags=ddo.metadata.tags,
        freely_available=False,
    )


def read_custom_data_file(logger: Logger, args: Args) -> Tuple[str, Config]:
    try:
        custom_data = CustomData.read_from_json_file(args.custom_data_file)
        file_extension = custom_data.fileInfo.fileExtension
        logger.info("File extension according to custom data file: %s", file_extension)
        config = custom_data.config
    except FileNotFoundError:
        # TODO Catch error about a missing custom data file but only until this file is generated by Pontux-X!
        logger.warning("Custom data file is missing, assuming 'csv' file extension...")
        file_extension = "csv"
        config = Config()
    return file_extension, config


async def run_service(logger: Logger, args: Args):
    ddo = read_ddo_file(args.ddo_file)
    logger.debug("DDO: %s", ddo)
    user_provided_edp_data = to_user_provided_edp_data(ddo)
    logger.debug("UserProvidedEdpData: %s", user_provided_edp_data)
    file_extension, edps_config = read_custom_data_file(logger, args)

    file_extension = sanitize_file_part(file_extension)
    input_filename = f"data.{file_extension}"

    with TemporaryDirectory() as temp_working_dir_path:
        ctx = TaskContextImpl(edps_config, logger, Path(temp_working_dir_path))

        ctx.input_path.mkdir(parents=True, exist_ok=True)
        shutil.copy(args.raw_data_file, ctx.input_path / input_filename)

        dump_service_info()
        logger.info("Processing asset..")
        await analyse_asset(ctx, user_provided_edp_data)

        logger.info("Zipping EDP..")
        main_ref = user_provided_edp_data.assetRefs[0]
        target_archive = args.output_dir / f"{sanitize_file_part(main_ref.assetId)}.zip"
        await ZipAlgorithm().compress(ctx.output_path, target_archive)
